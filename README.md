ğŸ§  Deep Learning Playground
Where neurons fire, weights adjust, and overfitting is just a phase.
Welcome to my deep learning sandbox â€” or should I say lab of controlled chaos? ğŸ¤–âœ¨
Here, models don't just learn, they evolve, sometimes cry in gradients, and occasionally throw a tantrum in the form of vanishing losses. This repo is my open diary of exploring the mysterious inner workings of neural networks â€” from basic building blocks to architectures that probably dream in tensors.

ğŸ§© Whatâ€™s Inside?
A delightful mix of:
Beginner-friendly notebooks (because no one is born knowing backprop)

Intermediate-level experiments (where I poke models till they behave)

Advanced architectures (a.k.a. the ones that require coffee and GPU sacrifices)

Failures (yes, theyâ€™re here too â€” because in DL, even your loss has value)

Remember: behind every great neural net is a human who Googled â€œwhy my model accuracy stuck at 50%â€ at least once.
Thanks for stopping by, and may your weights always converge! 

P.S. â€“ Okay, you caught me â€” I had a little help from ChatGPT writing thisğŸ˜…. But hey, it takes a neural network to truly understand the struggles, dreams, and existential dread of another neural network. Think of it as AI helping me decode AI â€” poetic, right? ğŸ˜„ğŸ˜„ğŸ˜„ğŸ˜„
