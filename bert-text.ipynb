{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets tokenizers\n!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n!unzip -qq cornell_movie_dialogs_corpus.zip\n!rm cornell_movie_dialogs_corpus.zip\n!mkdir datasets\n!mv cornell\\ movie-dialogs\\ corpus/movie_conversations.txt ./datasets\n!mv cornell\\ movie-dialogs\\ corpus/movie_lines.txt ./datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:43:05.770363Z","iopub.execute_input":"2025-05-12T19:43:05.770742Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nURL transformed to HTTPS due to an HSTS policy\n--2025-05-12 19:43:09--  https://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\nResolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.53\nConnecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.53|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9916637 (9.5M) [application/zip]\nSaving to: ‘cornell_movie_dialogs_corpus.zip’\n\ncornell_movie_dialo 100%[===================>]   9.46M  9.91MB/s    in 1.0s    \n\n2025-05-12 19:43:10 (9.91 MB/s) - ‘cornell_movie_dialogs_corpus.zip’ saved [9916637/9916637]\n\nreplace cornell movie-dialogs corpus/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport torch\nimport re\nimport random\nimport transformers, datasets\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer\nimport tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport itertools\nimport math\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.optim import Adam","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAX_LEN = 64\n\ncorpus_movie_conv = './datasets/movie_conversations.txt'\ncorpus_movie_lines = './datasets/movie_lines.txt'\nwith open(corpus_movie_conv, 'r', encoding='iso-8859-1') as c:\n    conv = c.readlines()\nwith open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n    lines = l.readlines()\n\nlines_dic = {}\nfor line in lines:\n    objects = line.split(\" +++$+++ \")\n    lines_dic[objects[0]] = objects[-1]\n\npairs = []\nfor con in conv:\n    ids = eval(con.split(\" +++$+++ \")[-1])\n    for i in range(len(ids)):\n        qa_pairs = []\n        \n        if i == len(ids) - 1:\n            break\n\n        first = lines_dic[ids[i]].strip()  \n        second = lines_dic[ids[i+1]].strip() \n\n        qa_pairs.append(' '.join(first.split()[:MAX_LEN]))\n        qa_pairs.append(' '.join(second.split()[:MAX_LEN]))\n        pairs.append(qa_pairs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(pairs[20])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.mkdir('./data')\ntext_data = []\nfile_count = 0\n\nfor sample in tqdm.tqdm([x[0] for x in pairs]):\n    text_data.append(sample)\n\n    # once we hit the 10K mark, save to file\n    if len(text_data) == 10000:\n        with open(f'./data/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n            fp.write('\\n'.join(text_data))\n        text_data = []\n        file_count += 1\n\npaths = [str(x) for x in Path('./data').glob('**/*.txt')]\n\n### training own tokenizer\ntokenizer = BertWordPieceTokenizer(\n    clean_text=True,\n    handle_chinese_chars=False,\n    strip_accents=False,\n    lowercase=True\n)\n\ntokenizer.train( \n    files=paths,\n    vocab_size=30_000, \n    min_frequency=5,\n    limit_alphabet=1000, \n    wordpieces_prefix='##',\n    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n    )\n\nos.mkdir('./bert-it-1')\ntokenizer.save_model('./bert-it-1', 'bert-it')\ntokenizer = BertTokenizer.from_pretrained('./bert-it-1/bert-it-vocab.txt', local_files_only=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BERTDataset(Dataset):\n    def __init__(self, data_pair, tokenizer, seq_len=64):\n\n        self.tokenizer = tokenizer\n        self.seq_len = seq_len\n        self.corpus_lines = len(data_pair)\n        self.lines = data_pair\n\n    def __len__(self):\n        return self.corpus_lines\n\n    def __getitem__(self, item):\n\n        t1, t2, is_next_label = self.get_sent(item)\n\n        t1_random, t1_label = self.random_word(t1)\n        t2_random, t2_label = self.random_word(t2)\n\n\n        t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n        t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']]\n        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n\n     \n        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n        bert_input = (t1 + t2)[:self.seq_len]\n        bert_label = (t1_label + t2_label)[:self.seq_len]\n        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(bert_input))]\n        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n\n        output = {\"bert_input\": bert_input,\n                  \"bert_label\": bert_label,\n                  \"segment_label\": segment_label,\n                  \"is_next\": is_next_label}\n\n        return {key: torch.tensor(value) for key, value in output.items()}\n\n    def random_word(self, sentence):\n        tokens = sentence.split()\n        output_label = []\n        output = []\n\n        for i, token in enumerate(tokens):\n            prob = random.random()\n\n            token_id = self.tokenizer(token)['input_ids'][1:-1]\n\n            if prob < 0.15:\n                prob /= 0.15\n\n                if prob < 0.8:\n                    for i in range(len(token_id)):\n                        output.append(self.tokenizer.vocab['[MASK]'])\n\n                elif prob < 0.9:\n                    for i in range(len(token_id)):\n                        output.append(random.randrange(len(self.tokenizer.vocab)))\n\n                else:\n                    output.append(token_id)\n\n                output_label.append(token_id)\n\n            else:\n                output.append(token_id)\n                for i in range(len(token_id)):\n                    output_label.append(0)\n\n        output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n        output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n        assert len(output) == len(output_label)\n        return output, output_label\n\n    def get_sent(self, index):\n        '''return random sentence pair'''\n        t1, t2 = self.get_corpus_line(index)\n\n        if random.random() > 0.5:\n            return t1, t2, 1\n        else:\n            return t1, self.get_random_line(), 0\n\n    def get_corpus_line(self, item):\n        '''return sentence pair'''\n        return self.lines[item][0], self.lines[item][1]\n\n    def get_random_line(self):\n        '''return random single sentence'''\n        return self.lines[random.randrange(len(self.lines))][1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = BERTDataset(\n   pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\ntrain_loader = DataLoader(\n   train_data, batch_size=32, shuffle=True, pin_memory=True)\nsample_data = next(iter(train_loader))\nprint(train_data[random.randrange(len(train_data))])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionalEmbedding(torch.nn.Module):\n\n    def __init__(self, d_model, max_len=128):\n        super().__init__()\n\n        pe = torch.zeros(max_len, d_model).float()\n        pe.require_grad = False\n\n        for pos in range(max_len):   \n            for i in range(0, d_model, 2):   \n                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n\n        self.pe = pe.unsqueeze(0)   \n\n    def forward(self, x):\n        return self.pe\n\nclass BERTEmbedding(torch.nn.Module):\n    \"\"\"\n    BERT Embedding which is consisted with under features\n        1. TokenEmbedding : normal embedding matrix\n        2. PositionalEmbedding : adding positional information using sin, cos\n        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n        sum of all these features are output of BERTEmbedding\n    \"\"\"\n\n    def __init__(self, vocab_size, embed_size, seq_len=64, dropout=0.1):\n        \"\"\"\n        :param vocab_size: total vocab size\n        :param embed_size: embedding size of token embedding\n        :param dropout: dropout rate\n        \"\"\"\n\n        super().__init__()\n        self.embed_size = embed_size\n        # (m, seq_len) --> (m, seq_len, embed_size)\n        # padding_idx is not updated during training, remains as fixed pad (0)\n        self.token = torch.nn.Embedding(vocab_size, embed_size, padding_idx=0)\n        self.segment = torch.nn.Embedding(3, embed_size, padding_idx=0)\n        self.position = PositionalEmbedding(d_model=embed_size, max_len=seq_len)\n        self.dropout = torch.nn.Dropout(p=dropout)\n       \n    def forward(self, sequence, segment_label):\n        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n        return self.dropout(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiHeadedAttention(torch.nn.Module):\n    \n    def __init__(self, heads, d_model, dropout=0.1):\n        super(MultiHeadedAttention, self).__init__()\n        \n        assert d_model % heads == 0\n        self.d_k = d_model // heads\n        self.heads = heads\n        self.dropout = torch.nn.Dropout(dropout)\n\n        self.query = torch.nn.Linear(d_model, d_model)\n        self.key = torch.nn.Linear(d_model, d_model)\n        self.value = torch.nn.Linear(d_model, d_model)\n        self.output_linear = torch.nn.Linear(d_model, d_model)\n        \n    def forward(self, query, key, value, mask):\n        \"\"\"\n        query, key, value of shape: (batch_size, max_len, d_model)\n        mask of shape: (batch_size, 1, 1, max_words)\n        \"\"\"\n        query = self.query(query)\n        key = self.key(key)        \n        value = self.value(value)   \n        \n        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n        \n        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n\n\n        scores = scores.masked_fill(mask == 0, -1e9)    \n\n\n        weights = F.softmax(scores, dim=-1)           \n        weights = self.dropout(weights)\n\n        context = torch.matmul(weights, value)\n\n        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n\n        return self.output_linear(context)\n\nclass FeedForward(torch.nn.Module):\n    \"Implements FFN equation.\"\n\n    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n        super(FeedForward, self).__init__()\n        \n        self.fc1 = torch.nn.Linear(d_model, middle_dim)\n        self.fc2 = torch.nn.Linear(middle_dim, d_model)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.activation = torch.nn.GELU()\n\n    def forward(self, x):\n        out = self.activation(self.fc1(x))\n        out = self.fc2(self.dropout(out))\n        return out\n\nclass EncoderLayer(torch.nn.Module):\n    def __init__(\n        self, \n        d_model=768,\n        heads=12, \n        feed_forward_hidden=768 * 4, \n        dropout=0.1\n        ):\n        super(EncoderLayer, self).__init__()\n        self.layernorm = torch.nn.LayerNorm(d_model)\n        self.self_multihead = MultiHeadedAttention(heads, d_model)\n        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n        self.dropout = torch.nn.Dropout(dropout)\n\n    def forward(self, embeddings, mask):\n\n        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n        # residual layer\n        interacted = self.layernorm(interacted + embeddings)\n        # bottleneck\n        feed_forward_out = self.dropout(self.feed_forward(interacted))\n        encoded = self.layernorm(feed_forward_out + interacted)\n        return encoded","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ScheduledOptim():\n    '''A simple wrapper class for learning rate scheduling'''\n\n    def __init__(self, optimizer, d_model, n_warmup_steps):\n        self._optimizer = optimizer\n        self.n_warmup_steps = n_warmup_steps\n        self.n_current_steps = 0\n        self.init_lr = np.power(d_model, -0.5)\n\n    def step_and_update_lr(self):\n        \"Step with the inner optimizer\"\n        self._update_learning_rate()\n        self._optimizer.step()\n\n    def zero_grad(self):\n        \"Zero out the gradients by the inner optimizer\"\n        self._optimizer.zero_grad()\n\n    def _get_lr_scale(self):\n        return np.min([\n            np.power(self.n_current_steps, -0.5),\n            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n\n    def _update_learning_rate(self):\n        ''' Learning rate scheduling per step '''\n\n        self.n_current_steps += 1\n        lr = self.init_lr * self._get_lr_scale()\n\n        for param_group in self._optimizer.param_groups:\n            param_group['lr'] = lr\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BERT(torch.nn.Module):\n    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n\n\n        super().__init__()\n        self.d_model = d_model\n        self.n_layers = n_layers\n        self.heads = heads\n\n        self.feed_forward_hidden = d_model * 4\n\n        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=d_model)\n\n        self.encoder_blocks = torch.nn.ModuleList(\n            [EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])\n\n    def forward(self, x, segment_info):\n\n        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n\n        x = self.embedding(x, segment_info)\n\n        for encoder in self.encoder_blocks:\n            x = encoder.forward(x, mask)\n        return x\n\nclass NextSentencePrediction(torch.nn.Module):\n\n\n    def __init__(self, hidden):\n\n        super().__init__()\n        self.linear = torch.nn.Linear(hidden, 2)\n        self.softmax = torch.nn.LogSoftmax(dim=-1)\n\n    def forward(self, x):\n        # use only the first token which is the [CLS]\n        return self.softmax(self.linear(x[:, 0]))\n\nclass MaskedLanguageModel(torch.nn.Module):\n\n\n    def __init__(self, hidden, vocab_size):\n\n        super().__init__()\n        self.linear = torch.nn.Linear(hidden, vocab_size)\n        self.softmax = torch.nn.LogSoftmax(dim=-1)\n\n    def forward(self, x):\n        return self.softmax(self.linear(x))\n\nclass BERTLM(torch.nn.Module):\n\n\n    def __init__(self, bert: BERT, vocab_size):\n  \n\n        super().__init__()\n        self.bert = bert\n        self.next_sentence = NextSentencePrediction(self.bert.d_model)\n        self.mask_lm = MaskedLanguageModel(self.bert.d_model, vocab_size)\n\n    def forward(self, x, segment_label):\n        x = self.bert(x, segment_label)\n        return self.next_sentence(x), self.mask_lm(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BERTTrainer:\n    def __init__(\n        self, \n        model, \n        train_dataloader, \n        test_dataloader=None, \n        lr= 1e-4,\n        weight_decay=0.01,\n        betas=(0.9, 0.999),\n        warmup_steps=10000,\n        log_freq=10,\n        device='cuda'\n        ):\n\n        self.device = device\n        self.model = model\n        self.train_data = train_dataloader\n        self.test_data = test_dataloader\n\n        # Setting the Adam optimizer with hyper-param\n        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n        self.optim_schedule = ScheduledOptim(\n            self.optim, self.model.bert.d_model, n_warmup_steps=warmup_steps\n            )\n\n        # Using Negative Log Likelihood Loss function for predicting the masked_token\n        self.criterion = torch.nn.NLLLoss(ignore_index=0)\n        self.log_freq = log_freq\n        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n    \n    def train(self, epoch):\n        self.iteration(epoch, self.train_data)\n\n    def test(self, epoch):\n        self.iteration(epoch, self.test_data, train=False)\n\n    def iteration(self, epoch, data_loader, train=True):\n        \n        avg_loss = 0.0\n        total_correct = 0\n        total_element = 0\n        \n        mode = \"train\" if train else \"test\"\n\n        # progress bar\n        data_iter = tqdm.tqdm(\n            enumerate(data_loader),\n            desc=\"EP_%s:%d\" % (mode, epoch),\n            total=len(data_loader),\n            bar_format=\"{l_bar}{r_bar}\"\n        )\n\n        for i, data in data_iter:\n\n            # 0. batch_data will be sent into the device(GPU or cpu)\n            data = {key: value.to(self.device) for key, value in data.items()}\n\n            # 1. forward the next_sentence_prediction and masked_lm model\n            next_sent_output, mask_lm_output = self.model.forward(data[\"bert_input\"], data[\"segment_label\"])\n\n            # 2-1. NLL(negative log likelihood) loss of is_next classification result\n            next_loss = self.criterion(next_sent_output, data[\"is_next\"])\n\n            # 2-2. NLLLoss of predicting masked token word\n            # transpose to (m, vocab_size, seq_len) vs (m, seq_len)\n            # criterion(mask_lm_output.view(-1, mask_lm_output.size(-1)), data[\"bert_label\"].view(-1))\n            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"])\n\n            # 2-3. Adding next_loss and mask_loss : 3.4 Pre-training Procedure\n            loss = next_loss + mask_loss\n\n            # 3. backward and optimization only in train\n            if train:\n                self.optim_schedule.zero_grad()\n                loss.backward()\n                self.optim_schedule.step_and_update_lr()\n\n            # next sentence prediction accuracy\n            correct = next_sent_output.argmax(dim=-1).eq(data[\"is_next\"]).sum().item()\n            avg_loss += loss.item()\n            total_correct += correct\n            total_element += data[\"is_next\"].nelement()\n\n            post_fix = {\n                \"epoch\": epoch,\n                \"iter\": i,\n                \"avg_loss\": avg_loss / (i + 1),\n                \"avg_acc\": total_correct / total_element * 100,\n                \"loss\": loss.item()\n            }\n\n            if i % self.log_freq == 0:\n                data_iter.write(str(post_fix))\n        print(\n            f\"EP{epoch}, {mode}: \\\n            avg_loss={avg_loss / len(data_iter)}, \\\n            total_acc={total_correct * 100.0 / total_element}\"\n        ) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = BERTDataset(\n   pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\n\ntrain_loader = DataLoader(\n   train_data, batch_size=32, shuffle=True, pin_memory=True)\n\nbert_model = BERT(\n  vocab_size=len(tokenizer.vocab),\n  d_model=768,\n  n_layers=2,\n  heads=12,\n  dropout=0.1\n)\n\nbert_lm = BERTLM(bert_model, len(tokenizer.vocab))\nbert_trainer = BERTTrainer(bert_lm, train_loader, device='cpu')\nepochs = 20\n\nfor epoch in range(epochs):\n  bert_trainer.train(epoch)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}